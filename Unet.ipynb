{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import time\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import torch.optim as optim\n",
    "from torch import optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from matplotlib import pyplot as plt\n",
    "from albumentations import (Resize, RandomCrop,VerticalFlip, HorizontalFlip, Normalize, Compose, CLAHE, Rotate)\n",
    "from albumentations.pytorch import ToTensor\n",
    "from torch.autograd import Variable\n",
    "from PIL import Image\n",
    "import segmentation_models_pytorch as smp\n",
    "import imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICE\"] = '0'\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transforms(phase, mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)): \n",
    "    list_transforms = [] \n",
    "    if phase == \"train\": \n",
    "        list_transforms.extend( \n",
    "            [ \n",
    "                HorizontalFlip(), \n",
    "                VerticalFlip(),\n",
    "                Rotate(),\n",
    "            ] ) \n",
    "    list_transforms.extend( [Resize(480, 480, interpolation=Image.BILINEAR),CLAHE(), Normalize(mean=mean, std=std, p=1), ToTensor(),] ) \n",
    "    list_trfms = Compose(list_transforms) \n",
    "    return list_trfms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readImg(im_fn):\n",
    "    im = cv2.imread(im_fn)\n",
    "    if im is None :\n",
    "        tmp = imageio.mimread(im_fn)\n",
    "        if tmp is not None:\n",
    "            im = np.array(tmp)\n",
    "            im = im.transpose(1,2,0)\n",
    "        else:\n",
    "            image = Image.open(im_fn)\n",
    "            im = np.asarray(image)\n",
    "    else:\n",
    "        im = cv2.cvtColor(np.asarray(im), cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    return im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RetinalDataset(Dataset):\n",
    "    def __init__(self, name, img_root, gt_root, phase):\n",
    "        super().__init__()\n",
    "        self.inputs = []\n",
    "        self.gts = []\n",
    "        self.transform = get_transforms(phase)\n",
    "        \n",
    "        for root in img_root:\n",
    "            file_list = os.getcwd() + root\n",
    "            list_image = os.listdir(file_list)\n",
    "            list_image.sort()\n",
    "\n",
    "            for i, image_path in enumerate(list_image):\n",
    "                img = os.path.join(file_list,list_image[i])\n",
    "                self.inputs.append(img)\n",
    "                \n",
    "        for root in gt_root:\n",
    "            file_list = os.getcwd() + root\n",
    "            list_image = os.listdir(file_list)\n",
    "            list_image.sort()\n",
    "\n",
    "            for i, image_path in enumerate(list_image):\n",
    "                img = os.path.join(file_list,list_image[i])\n",
    "                self.gts.append(img)\n",
    "\n",
    "        print('Load %s: %d samples for %s'%(name, len(self.inputs),phase))\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        image = readImg(self.inputs[index])\n",
    "        mask = readImg(self.gts[index])\n",
    "        if mask.shape[2] == 3:\n",
    "            mask = mask[:,:,0]\n",
    "            \n",
    "        augmented = self.transform(image=image, mask=mask.squeeze()) \n",
    "        return augmented[\"image\"], augmented[\"mask\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DRIVE 数据集\n",
    "dr_train_loader = RetinalDataset('DRIVE',['\\\\data\\\\DRIVE\\\\train\\\\images'],\n",
    "                           ['\\\\data\\\\DRIVE\\\\train\\\\1st_manual'], 'train')\n",
    "dr_test_loader = RetinalDataset('DRIVE',['\\\\data\\\\DRIVE\\\\test\\\\images'],\n",
    "                           ['\\\\data\\\\DRIVE\\\\test\\\\1st_manual'], 'test')\n",
    "\n",
    "# STARE 数据集\n",
    "st_train_loader = RetinalDataset('STARE',['\\\\data\\\\STARE\\\\train\\\\image'],\n",
    "                           ['\\\\data\\\\STARE\\\\train\\\\labels-ah'], 'train')\n",
    "st_test_loader = RetinalDataset('STARE',['\\\\data\\\\STARE\\\\test\\\\image'],\n",
    "                           ['\\\\data\\\\STARE\\\\test\\\\labels-ah'], 'test')\n",
    "\n",
    "# CHASEDB1 数据集\n",
    "st_train_loader = RetinalDataset('CHASEDB1',['\\\\data\\\\CHASEDB1\\\\train\\\\image'],\n",
    "                           ['\\\\data\\\\CHASEDB1\\\\train\\\\1st'], 'train')\n",
    "st_test_loader = RetinalDataset('CHASEDB1',['\\\\data\\\\CHASEDB1\\\\test\\\\image'],\n",
    "                           ['\\\\data\\\\CHASEDB1\\\\test\\\\1st'], 'test')\n",
    "\n",
    "# HRF 数据集\n",
    "hr_train_loader = RetinalDataset('HRF',['\\\\data\\\\HRF\\\\train\\\\images'],\n",
    "                           ['\\\\data\\\\HRF\\\\train\\\\manual1'], 'train')\n",
    "hr_test_loader = RetinalDataset('HRF',['\\\\data\\\\HRF\\\\test\\\\images'],\n",
    "                           ['\\\\data\\\\HRF\\\\test\\\\manual1'], 'test')\n",
    "\n",
    "# 混合训练集\n",
    "all_train_loader = RetinalDataset('all',['\\\\data\\\\DRIVE\\\\train\\\\images','\\\\data\\\\STARE\\\\train\\\\image',\n",
    "                                        '\\\\data\\\\CHASEDB1\\\\train\\\\image','\\\\data\\\\HRF\\\\train\\\\images'],\n",
    "                                 ['\\\\data\\\\DRIVE\\\\train\\\\1st_manual','\\\\data\\\\STARE\\\\train\\\\labels-ah',\n",
    "                                 '\\\\data\\\\CHASEDB1\\\\train\\\\1st','\\\\data\\\\HRF\\\\train\\\\manual1'],'train')\n",
    "\n",
    "all_test_loader = RetinalDataset('all',['\\\\data\\\\DRIVE\\\\test\\\\images','\\\\data\\\\STARE\\\\test\\\\image',\n",
    "                                        '\\\\data\\\\CHASEDB1\\\\test\\\\image','\\\\data\\\\HRF\\\\test\\\\images'],\n",
    "                                 ['\\\\data\\\\DRIVE\\\\test\\\\1st_manual','\\\\data\\\\STARE\\\\test\\\\labels-ah',\n",
    "                                 '\\\\data\\\\CHASEDB1\\\\test\\\\1st','\\\\data\\\\HRF\\\\test\\\\manual1'],'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "epochs = 500\n",
    "lr = 0.001\n",
    "batch_iter = math.ceil(len(all_train_loader) / batch_size)\n",
    "net = smp.Unet('resnet18', classes=1, activation=None, encoder_weights='imagenet')\n",
    "net.cuda()\n",
    "\n",
    "net_name = 'Unet-Resnet18'\n",
    "loss_fuc = 'BCEL'\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=lr)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode=\"min\", patience=4, verbose=True)\n",
    "\n",
    "dataset = \"all\"\n",
    "trainloader = DataLoader(all_train_loader, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "testloader = DataLoader(all_test_loader, batch_size=1, shuffle=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_path = 'results'\n",
    "if not os.path.exists(result_path):\n",
    "    os.makedirs(result_path)\n",
    "weights_path =  \"weights\"\n",
    "if not os.path.exists(weights_path):\n",
    "    os.makedirs(weights_path)\n",
    "image_path = os.path.join(result_path,dataset)\n",
    "if not os.path.exists(image_path):\n",
    "    os.makedirs(image_path)\n",
    "\n",
    "f_loss = open(os.path.join(result_path, \"log_%s_%s_%s.txt\"%(dataset,loss_fuc,net_name)),'w')\n",
    "f_loss.write('Dataset : %s\\n'%dataset)\n",
    "f_loss.write('Loss : %s\\n'%loss_fuc)\n",
    "f_loss.write('Net : %s\\n'%net_name)\n",
    "f_loss.write('Learning rate: %05f\\n'%lr)\n",
    "f_loss.write('batch-size: %s\\n'%batch_size)\n",
    "f_loss.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(e):\n",
    "    print('start train epoch: %d'%e)\n",
    "    net.train()\n",
    "    \n",
    "    loss_plot = []\n",
    "    \n",
    "    for i, (x,y) in enumerate(trainloader):\n",
    "        optimizer.zero_grad()\n",
    "        x = x.cuda(async=True)\n",
    "        y = y.cuda(async=True)\n",
    "        \n",
    "        x = net(x)\n",
    "        \n",
    "        loss = criterion(x.squeeze(), y.squeeze())\n",
    "        print('Epoch:%d  Batch:%d/%d  loss:%08f'%(e, i+1, batch_iter, loss.data))\n",
    "        \n",
    "        loss_plot.append(loss.item())\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return loss_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    net.eval()\n",
    "    acc = torch.tensor(0)\n",
    "    tpr = torch.tensor(0)\n",
    "    fpr = torch.tensor(0)\n",
    "    sn = torch.tensor(0)\n",
    "    sp = torch.tensor(0)\n",
    "    \n",
    "    \n",
    "    for i, (x,y) in enumerate(testloader):\n",
    "        optimizer.zero_grad()\n",
    "        x = x.cuda(async=True)\n",
    "        y = y.cuda(async=True)\n",
    "        x = net(x)\n",
    "        \n",
    "        x = torch.sigmoid(x).squeeze()\n",
    "        y = y.squeeze().int().long()\n",
    "        \n",
    "        x = torch.where(x > 0.5, torch.tensor(1).cuda(), torch.tensor(0).cuda())\n",
    "        \n",
    "        temp = x + torch.tensor(2).cuda().long() * y\n",
    "        tp = torch.sum(torch.where(temp == 3, torch.tensor(1).cuda(),torch.tensor(0).cuda())).float()\n",
    "        fp = torch.sum(torch.where(temp == 1, torch.tensor(1).cuda(),torch.tensor(0).cuda())).float()\n",
    "        tn = torch.sum(torch.where(temp == 0, torch.tensor(1).cuda(),torch.tensor(0).cuda())).float()\n",
    "        fn = torch.sum(torch.where(temp == 2, torch.tensor(1).cuda(),torch.tensor(0).cuda())).float()\n",
    "        \n",
    "        acc = acc + (tp + tn) / (tp + fp + tn + fn)\n",
    "        tpr = tpr + tp / (tp + fn)\n",
    "        fpr = fpr + fp / (tn + fp)\n",
    "        sn = sn + tn / (tn + fp)\n",
    "        sp = sp + tp / (tp + fn)\n",
    "        \n",
    "    acc = (acc / len(testloader)).cpu().numpy()\n",
    "    tpr = (tpr / len(testloader)).cpu().numpy()\n",
    "    fpr = (fpr / len(testloader)).cpu().numpy()\n",
    "    sn = (sn / len(testloader)).cpu().numpy()\n",
    "    sp = (sp / len(testloader)).cpu().numpy()\n",
    "    \n",
    "    print('ACC:',acc)\n",
    "    print('TPR:',tpr)\n",
    "    print('FPR:',fpr)\n",
    "    print('SN:',sn)\n",
    "    print('SP:',sp)\n",
    "    \n",
    "    f_log = open(os.path.join(result_path, \"log_%s_%s_%s.txt\"%(dataset,loss_fuc,net_name)),'a')\n",
    "    f_log.write('Epoch:%d  acc:%08f\\n'%(e, acc))\n",
    "    f_log.write('Epoch:%d  TPR:%08f\\n'%(e, tpr))\n",
    "    f_log.write('Epoch:%d  FPR:%08f\\n'%(e, fpr))\n",
    "    f_log.write('Epoch:%d  SN:%08f\\n'%(e, sn))\n",
    "    f_log.write('Epoch:%d  SP:%08f\\n'%(e, sp))\n",
    "    f_log.close()     \n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_acc = 0\n",
    "loss_plot = [0]\n",
    "for e in range(1, epochs + 1):\n",
    "    loss_plot = loss_plot + train(e)\n",
    "    if e % 10 == 0:\n",
    "        acc = test()\n",
    "        if acc > best_acc:\n",
    "            if best_acc != 0:\n",
    "                os.remove(os.path.join(weights_path,\n",
    "                         'net_%s_%s_%s_%f.pth'%(dataset,loss_fuc,net_name,best_acc)))\n",
    "            torch.save(net.state_dict(),os.path.join(weights_path,\n",
    "                         'net_%s_%s_%s_%f.pth'%(dataset,loss_fuc,net_name,acc)))  \n",
    "            best_acc = acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_plot[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_plot():\n",
    "    net.eval()\n",
    "    res = []\n",
    "    for i, (x,y) in enumerate(testloader):\n",
    "        optimizer.zero_grad()\n",
    "        x = x.cuda(async=True)\n",
    "        y = y.cuda(async=True)\n",
    "        x = net(x)\n",
    "        \n",
    "        x = torch.sigmoid(x).squeeze()\n",
    "        y = y.squeeze().int().long().cpu().detach().numpy()\n",
    "        \n",
    "        x = torch.where(x > 0.5, torch.tensor(1).cuda(), torch.tensor(0).cuda()).cpu().detach().numpy()\n",
    "        \n",
    "        acc = np.sum(np.where(x == y,1,0)) / np.sum(np.where(x == x,1,0))\n",
    "        res.append(acc)\n",
    "        im = cv2.merge([x*255,y*255,y*255])\n",
    "        plt.imsave(os.path.join(image_path,(str(i)+'_'+'%4f'%acc+'.png')),im.astype('uint8'), format=\"png\")\n",
    "        \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume = os.path.join(weights_path,\n",
    "                         'net_%s_%s_%s_%f.pth'%(dataset,loss_fuc,net_name,best_acc))\n",
    "pre_params = torch.load(resume)\n",
    "net.load_state_dict(pre_params)\n",
    "res = test_plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = np.array(res)\n",
    "print(np.mean(res[0:20])) # DRIVE\n",
    "print(np.mean(res[20:30])) # STARE\n",
    "print(np.mean(res[30:44])) # CHASEDB1\n",
    "print(np.mean(res[44:68])) # HRF"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pt] *",
   "language": "python",
   "name": "conda-env-pt-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}